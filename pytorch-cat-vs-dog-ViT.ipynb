{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.zenfs.com/en/usa_today_entertainment_893/557b114cf813d0a6e44a34b7e6a48eef\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Introduction***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a experiment with the kaggle competition [Dogs vs. Cats Redux: Kernels Edition](https://www.kaggle.com/competitions/dogs-vs-cats-redux-kernels-edition/overview) that ended 6 years ago. While its not really fair to compare my results to the top results of the competition, I wanted to test how well I could perform using a pretrained ViT model, and also explore the different ViT models that are available in the pytorch library.\n",
    "\n",
    "This is the first time I participated in a computer vision competition, so a big part of this notebook is me finding a way to work with images in pytorch, and also how to set up a complete training pipeline.\n",
    "\n",
    "In this notebook, I am focusing on two versions of the ViT model, the ViT-B_16 and the ViT-L_16. I wanted to explore how the sizes of the model would affect the results, and also how the sizes would affect the training time. \n",
    "\n",
    "As this is my first time working with images in PyTorch, I am sure there are many things that could be improved, especially in the training pipeline. But I will leave things as they are for now, and try to improve on them in future image classification projects.\n",
    "<br><br><br>\n",
    "***Beware:*** No type hints or docstrings can be found in this notebook! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install jupyter_black -q\n",
    "!pip install albumentations -q\n",
    "!pip install kaggle -q\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Imports***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.utils as utils\n",
    "import torchvision\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from time import time\n",
    "from sklearn.metrics import log_loss as logloss\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import re\n",
    "import cv2\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "os.environ[\"KAGGLE_USERNAME\"] = \"davidvikstrand\"\n",
    "os.environ[\"KAGGLE_KEY\"] = XX\n",
    "\n",
    "from colorama import Fore, Style\n",
    "\n",
    "rs = Style.RESET_ALL\n",
    "gr = Fore.GREEN\n",
    "rd = Fore.RED\n",
    "cy = Fore.CYAN\n",
    "ye = Fore.YELLOW\n",
    "ma = Fore.MAGENTA\n",
    "bl = Fore.BLUE\n",
    "gld = Fore.YELLOW + Style.BRIGHT\n",
    "wh = Fore.WHITE + Style.BRIGHT\n",
    "\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Constants***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 420\n",
    "batch_size = 32\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "num_outp = 1\n",
    "device = \"cuda\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Set Up***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatDog(Dataset):\n",
    "    def __init__(self, root=\"train\", transform=None):\n",
    "        self.images = os.listdir(root)\n",
    "        self.images.sort(key=lambda x: int(re.findall(r\"\\d+\", x)[0]))\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = self.images[idx]\n",
    "        img = Image.open(os.path.join(self.root, file))\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(image=np.array(img))[\"image\"]\n",
    "\n",
    "        label = 1 if \"dog\" in file else 0 if \"cat\" in file else -1\n",
    "        return img, label, file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dense(img_s=img_s):\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.RandomResizedCrop(\n",
    "                p=1.0,\n",
    "                height=img_s,\n",
    "                width=img_s,\n",
    "                scale=(0.7, 1.2),\n",
    "                ratio=(0.75, 1.3),\n",
    "                interpolation=1,\n",
    "            ),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.ColorJitter(\n",
    "                brightness=0.3, contrast=0.3, saturation=0.3, hue=0.35, p=0.7\n",
    "            ),\n",
    "            A.ShiftScaleRotate(\n",
    "                shift_limit=0.0,\n",
    "                scale_limit=0.1,\n",
    "                rotate_limit=30,\n",
    "                interpolation=cv2.INTER_LINEAR,\n",
    "                border_mode=cv2.BORDER_CONSTANT,\n",
    "                p=0.8,\n",
    "            ),\n",
    "            A.Blur(blur_limit=(1, 3), p=0.25),\n",
    "            A.CoarseDropout(max_holes=2, max_height=50, max_width=50, p=0.5),\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "                max_pixel_value=255.0,\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def transform_medium(img_s=img_s):\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(height=img_s, width=img_s),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.OneOf(\n",
    "                [\n",
    "                    A.ColorJitter(\n",
    "                        brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, p=1\n",
    "                    ),\n",
    "                    A.CoarseDropout(max_holes=1, max_height=10, max_width=10, p=1),\n",
    "                ],\n",
    "                p=0.7,\n",
    "            ),\n",
    "            A.ShiftScaleRotate(\n",
    "                shift_limit=0.1,\n",
    "                scale_limit=0.1,\n",
    "                rotate_limit=20,\n",
    "                interpolation=cv2.INTER_LINEAR,\n",
    "                border_mode=cv2.BORDER_CONSTANT,\n",
    "                p=0.8,\n",
    "            ),\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "                max_pixel_value=255.0,\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def transform_light(img_s=img_s):\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(height=img_s, width=img_s),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.ShiftScaleRotate(\n",
    "                shift_limit=0.0,\n",
    "                scale_limit=0.1,\n",
    "                rotate_limit=15,\n",
    "                interpolation=cv2.INTER_LINEAR,\n",
    "                border_mode=cv2.BORDER_CONSTANT,\n",
    "                p=0.8,\n",
    "            ),\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "                max_pixel_value=255.0,\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def basic_transform(img_s=img_s):\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(height=img_s, width=img_s),\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "                max_pixel_value=255.0,\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Models***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vit_b_16_swag-9ac1b537.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16_swag-9ac1b537.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d009594be5494133b3d66e2c9524b27f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_vit_b_16(n_out=1):\n",
    "    global img_s\n",
    "    img_s = 384\n",
    "\n",
    "    vit_weights = torchvision.models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1\n",
    "\n",
    "    ViT = torchvision.models.vit_b_16(weights=vit_weights)\n",
    "    ViT.heads = nn.Linear(in_features=768, out_features=n_out)\n",
    "\n",
    "    for param in ViT.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in ViT.heads.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    return ViT.to(device)\n",
    "\n",
    "\n",
    "load_vit_b_16();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vit_l_16_swag-4f3808c9.pth\" to /root/.cache/torch/hub/checkpoints/vit_l_16_swag-4f3808c9.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f272fc1662246de87ce49cc7e85f030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/1.14G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_vit_l_16(n_out=1):\n",
    "    global img_s\n",
    "    img_s = 512\n",
    "\n",
    "    vit_weights = torchvision.models.ViT_L_16_Weights.IMAGENET1K_SWAG_E2E_V1\n",
    "\n",
    "    ViT = torchvision.models.vit_l_16(weights=vit_weights)\n",
    "    ViT.heads = nn.Linear(in_features=1024, out_features=n_out)\n",
    "\n",
    "    for param in ViT.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in ViT.heads.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    return ViT.to(device)\n",
    "\n",
    "\n",
    "load_vit_l_16();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble(nn.Module):\n",
    "    def __init__(self, models):\n",
    "        super(Ensemble, self).__init__()\n",
    "        self.models = nn.ModuleList(models)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = [model(x) for model in self.models]\n",
    "        return torch.stack(outputs).mean(dim=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Utils***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some utility functions that I will use in the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p(train, i, loss, acc, log_loss):\n",
    "    if train:\n",
    "        print(\n",
    "            f\"{i}: [Train] Loss: {bl}{np.mean(loss):.4f}{rs}, \"\n",
    "            f\"Accuracy: {bl}{np.mean(acc):.4f}{rs}\",\n",
    "            f\"log_loss: {bl}{np.mean(log_loss):.4f}{rs}\",\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"[Validation] Loss: {gr}{np.mean(loss):.4f}{rs}, \"\n",
    "            f\"Accuracy: {gr}{np.mean(acc):.4f}{rs},\",\n",
    "            f\"Log Loss: {gr}{np.mean(log_loss):.4f}{rs}\\n\",\n",
    "        )\n",
    "\n",
    "\n",
    "def get_scores(logs, labels):\n",
    "    pred = torch.sigmoid(logs)\n",
    "    acc = ((pred > 0.5) == labels).sum() / pred.size(0)\n",
    "\n",
    "    pred = torch.clip(pred, 0.005, 0.995).cpu().detach().numpy()\n",
    "    log_loss = torch.tensor(logloss(labels.cpu().numpy(), pred))\n",
    "    return acc, log_loss\n",
    "\n",
    "\n",
    "def focal_loss(logits, targets, criterion, alpha=0.5, gamma=2):\n",
    "    loss_score = criterion(logits, targets)\n",
    "\n",
    "    prob = torch.sigmoid(logits)\n",
    "    factor = (1 - prob) ** gamma\n",
    "    loss = alpha * factor * loss_score\n",
    "\n",
    "    return loss.mean()\n",
    "\n",
    "def empty_cache():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def make_predictions(model, loader, batch_size=batch_size):\n",
    "    print(\"Predicting...\")\n",
    "    model.eval()\n",
    "    all_preds = torch.empty((len(loader.dataset), 1))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (data, *_) in enumerate(tqdm(loader)):\n",
    "            outputs = model(data.to(device))\n",
    "            all_preds[i * batch_size : (i + 1) * batch_size] = outputs.sigmoid()\n",
    "\n",
    "    return all_preds.squeeze()\n",
    "\n",
    "\n",
    "def get_model(model, lr, wd, scheduler):\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        weight_decay=wd,\n",
    "        amsgrad=True,\n",
    "    )\n",
    "    if len(scheduler) == 2:\n",
    "        scheduler, scheduler_params = scheduler\n",
    "        scheduler = scheduler(optimizer, **scheduler_params)\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    return model, optimizer, scheduler\n",
    "\n",
    "\n",
    "def set_seed(seed=420):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def remove_seed():\n",
    "    s = lambda: random.randint(0, 1e8)\n",
    "    torch.manual_seed(s())\n",
    "    torch.cuda.manual_seed_all(s())\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    np.random.seed()\n",
    "    random.seed()\n",
    "\n",
    "\n",
    "def early_stopping(\n",
    "    stopping, best_loss, best_state, val_loss, model, patience, cur_epoch\n",
    "):\n",
    "    stopping += 1\n",
    "    if val_loss <= best_loss:\n",
    "        best_loss, stopping = val_loss, 0\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    if stopping >= patience:\n",
    "        print(f\"{Fore.RED}Early Stopping at epoch {cur_epoch+1}{rs}\")\n",
    "\n",
    "    return stopping, best_loss, best_state, stopping >= patience\n",
    "\n",
    "def submit_predictions(preds):\n",
    "    test_preds = np.clip(preds, 0.005, 0.995)\n",
    "\n",
    "    submission_df = pd.DataFrame(\n",
    "        {\"id\": range(1, len(test_preds) + 1), \"label\": test_preds}\n",
    "    )\n",
    "    submission_df.to_csv(\"submission.csv\", index=False)\n",
    "    !kaggle competitions submit -c dogs-vs-cats-redux-kernels-edition -f submission.csv -m \"Message\"\n",
    "\n",
    "\n",
    "def get_data(train_amn, val_amn, model):\n",
    "    img_s = {\"vit_b_16\": 384, \"vit_l_16\": 512}[model]\n",
    "    train_dataset = CatDog(root=\"train/\", transform=transform_light(img_s))\n",
    "    test_dataset = CatDog(root=\"test/\", transform=basic_transform(img_s))\n",
    "\n",
    "    val_s = int(val_amn * (tr_len := len(train_dataset)))\n",
    "    train_set, val_set = random_split(train_dataset, [tr_len - val_s, val_s])\n",
    "    val_set = copy.deepcopy(val_set)\n",
    "    val_set.dataset.transform = basic_transform(img_s)\n",
    "\n",
    "    train_s = int(train_amn * (tr_len := len(train_set)))\n",
    "    train_set, _ = random_split(train_set, [train_s, tr_len - train_s])\n",
    "\n",
    "    return train_set, val_set, test_dataset\n",
    "\n",
    "\n",
    "def get_loader(dataset, bs, shuffle=False):\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        shuffle=shuffle,\n",
    "        batch_size=bs,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "def set_transform(data_set, transf_d, epoch, bs):\n",
    "    transf = [transf_d[k] for k in transf_d if epoch in k] or [transform_light]\n",
    "\n",
    "    data_set = copy.deepcopy(data_set)\n",
    "    data_set.dataset.transform = transf[0](img_s)\n",
    "    return get_loader(data_set, bs, shuffle=True)\n",
    "\n",
    "\n",
    "def one_epoch(loader, train, model, criterion, optimizer, scaler, focal=False):\n",
    "    model.train(train)\n",
    "\n",
    "    metrics = [], [], []\n",
    "    for i, (data, labels, _) in enumerate(tqdm(loader), 1):\n",
    "        data, labels = data.to(device), labels.to(device).unsqueeze(1).float()\n",
    "\n",
    "        if labels.unique().numel() == 1:\n",
    "            continue\n",
    "\n",
    "        if train:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logs = model(data)\n",
    "                if focal:\n",
    "                    loss_score = focal_loss(logs, labels, criterion)\n",
    "                else:\n",
    "                    loss_score = criterion(logs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss_score).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                logs = model(data)\n",
    "                loss_score = criterion(logs, labels)\n",
    "\n",
    "        for j, item in enumerate((loss_score, *get_scores(logs, labels))):\n",
    "            metrics[j].append(item.item())\n",
    "\n",
    "    p(train, i, *metrics)\n",
    "    return np.mean(metrics[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Main***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not an optimal implementation, I will definitely improve on this in future projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model_fn, data, epochs, deterministic, params, transform_d, load_best_state\n",
    "):\n",
    "    start = time()\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "    set_seed() if deterministic else remove_seed()\n",
    "\n",
    "    wd, lr, bs, bs_val, patience, focal, warm_up, *scheduler = params.values()\n",
    "    model, optimizer, scheduler = get_model(model_fn(), lr, wd, scheduler)\n",
    "\n",
    "    print(\n",
    "        f\"Parameters: {bl}batch_size={bs}, weight_decay={wd}\",\n",
    "        f\"learning_rate={lr}{rs}\\n\",\n",
    "    )\n",
    "\n",
    "    train_data, val_data, test_data = data\n",
    "    val_loader, test_loader = [\n",
    "        get_loader(data, bs_val) for data in (val_data, test_data)\n",
    "    ]\n",
    "\n",
    "    stopping, best_loss, best_state = 0, np.inf, None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"{wh}[Epoch {epoch+1}/{epochs}]{rs}\")\n",
    "\n",
    "        train_args = [model, criterion, optimizer, scaler]\n",
    "        train_loader = set_transform(train_data, transform_d, epoch, bs)\n",
    "\n",
    "        one_epoch(train_loader, True, *train_args, epoch in focal)\n",
    "        val_loss = one_epoch(val_loader, False, *train_args)\n",
    "        empty_cache()\n",
    "\n",
    "        if scheduler is not None and epoch not in [warm_up, epochs - 1]:\n",
    "            scheduler.step()\n",
    "            print(f\"Updated lr -> {ma}{scheduler.get_last_lr()[0]:.2e}{rs}\\n\")\n",
    "\n",
    "        stopping, best_loss, best_state, early_stop = early_stopping(\n",
    "            stopping, best_loss, best_state, val_loss, model, patience, epoch\n",
    "        )\n",
    "\n",
    "        if early_stop and epoch != epochs - 1:\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_state) if load_best_state else None\n",
    "    predictions = make_predictions(model, test_loader, bs_val)\n",
    "    submit_predictions(predictions)\n",
    "    empty_cache()\n",
    "\n",
    "    print(f\"\\n{bl}(ﾉ´ヮ`)ﾉ*  Finished in {(time() - start) / 60:.2f} minutes{rs}\")\n",
    "    return model, predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Training***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inp = {\n",
    "    \"train_amn\": 1,\n",
    "    \"val_amn\": 0.2,\n",
    "}\n",
    "\n",
    "data_vit_b_16 = get_data(**data_inp, model=\"vit_b_16\")\n",
    "data_vit_l_16 = get_data(**data_inp, model=\"vit_l_16\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***ViT-B_16***\n",
    "\n",
    "Here are all the parameters that needs to be set before training.\n",
    "\n",
    "The **transform_d** parameter is a dictionary that contains the transforms that will be applied during training and validation. The keys are which epoch the transforms should be applied, and the values are the transforms. In this example, the transform_light is applied for epoch 0-3 and transform_medium for epoch 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: \u001b[34mbatch_size=32, weight_decay=5e-05 learning_rate=0.002\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[1m[Epoch 1/4]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624/624 [00:51<00:00, 12.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624: [Train] Loss: \u001b[34m0.0123\u001b[0m, Accuracy: \u001b[34m0.9961\u001b[0m log_loss: \u001b[34m0.0159\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:43<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: \u001b[32m0.0036\u001b[0m, Accuracy: \u001b[32m0.9990\u001b[0m, Log Loss: \u001b[32m0.0082\u001b[0m\n",
      "\n",
      "Updated lr -> \u001b[35m1.40e-03\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[1m[Epoch 2/4]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624/624 [00:50<00:00, 12.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624: [Train] Loss: \u001b[34m0.0050\u001b[0m, Accuracy: \u001b[34m0.9989\u001b[0m log_loss: \u001b[34m0.0091\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:42<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: \u001b[32m0.0034\u001b[0m, Accuracy: \u001b[32m0.9990\u001b[0m, Log Loss: \u001b[32m0.0080\u001b[0m\n",
      "\n",
      "Updated lr -> \u001b[35m9.80e-04\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[1m[Epoch 3/4]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624/624 [00:50<00:00, 12.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624: [Train] Loss: \u001b[34m0.0007\u001b[0m, Accuracy: \u001b[34m0.9993\u001b[0m log_loss: \u001b[34m0.0072\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:42<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: \u001b[32m0.0032\u001b[0m, Accuracy: \u001b[32m0.9992\u001b[0m, Log Loss: \u001b[32m0.0079\u001b[0m\n",
      "\n",
      "Updated lr -> \u001b[35m6.86e-04\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[1m[Epoch 4/4]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624/624 [00:50<00:00, 12.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624: [Train] Loss: \u001b[34m0.0008\u001b[0m, Accuracy: \u001b[34m0.9991\u001b[0m log_loss: \u001b[34m0.0075\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:41<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: \u001b[32m0.0031\u001b[0m, Accuracy: \u001b[32m0.9992\u001b[0m, Log Loss: \u001b[32m0.0078\u001b[0m\n",
      "\n",
      "Predicting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [01:40<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 137k/137k [00:00<00:00, 316kB/s]\n",
      "Successfully submitted to Dogs vs. Cats Redux: Kernels Edition\n",
      "\u001b[34m(ﾉ´ヮ`)ﾉ*  Finished in 7.96 minutes\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_inp = {\n",
    "    \"model_fn\": load_vit_b_16,\n",
    "    \"data\": data_vit_b_16,\n",
    "    \"epochs\": 4,\n",
    "    \"deterministic\": False,\n",
    "    \"load_best_state\": False,\n",
    "    \"params\": {\n",
    "        \"weight_decay\": 5e-5,\n",
    "        \"learning_rate\": 2e-3,\n",
    "        \"bs_train\": 32,\n",
    "        \"bs_val\": 64,\n",
    "        \"patience\": np.inf,\n",
    "        \"focal\": [2, 3],\n",
    "        \"warm_up\": [0],\n",
    "        \"scheduler\": lr_scheduler.StepLR,\n",
    "        \"scheduler_args\": {\"step_size\": 1, \"gamma\": 0.7},\n",
    "    },\n",
    "    \"transform_d\": {\n",
    "        tuple(range(4)): transform_light,\n",
    "        tuple(range(4, 5)): transform_medium,\n",
    "        tuple(range(16, 21)): transform_dense,\n",
    "    },\n",
    "}\n",
    "\n",
    "model, predictions = train(**train_inp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vit_b](imgs/vit_b_16.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After just four epochs, the ViT-B_16 model has already reached a first place on the leaderboard with a score of 0.03210, beating the top score of the competition at 0.03302. With a training time of 6 minutes and prediction time of 2 minutes, this is a pretty good result. Now lets see if the ViT-L_16 can beat it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***ViT-L_16***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: \u001b[34mbatch_size=32, weight_decay=0.001 learning_rate=0.002\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[1m[Epoch 1/2]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624/624 [05:05<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624: [Train] Loss: \u001b[34m0.0100\u001b[0m, Accuracy: \u001b[34m0.9970\u001b[0m log_loss: \u001b[34m0.0134\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [04:18<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: \u001b[32m0.0029\u001b[0m, Accuracy: \u001b[32m0.9996\u001b[0m, Log Loss: \u001b[32m0.0071\u001b[0m\n",
      "\n",
      "Updated lr -> \u001b[35m1.80e-03\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[1m[Epoch 2/2]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624/624 [05:04<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624: [Train] Loss: \u001b[34m0.0006\u001b[0m, Accuracy: \u001b[34m0.9995\u001b[0m log_loss: \u001b[34m0.0067\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [04:17<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: \u001b[32m0.0025\u001b[0m, Accuracy: \u001b[32m0.9996\u001b[0m, Log Loss: \u001b[32m0.0069\u001b[0m\n",
      "\n",
      "Predicting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [10:42<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 137k/137k [00:00<00:00, 252kB/s]\n",
      "Successfully submitted to Dogs vs. Cats Redux: Kernels Edition\n",
      "\u001b[34m(ﾉ´ヮ`)ﾉ*  Finished in 29.59 minutes\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_inp = {\n",
    "    \"model_fn\": load_vit_l_16,\n",
    "    \"data\": data_vit_l_16,\n",
    "    \"epochs\": 2,\n",
    "    \"deterministic\": False,\n",
    "    \"load_best_state\": False,\n",
    "    \"params\": {\n",
    "        \"weight_decay\": 1e-3,\n",
    "        \"learning_rate\": 2e-3,\n",
    "        \"bs_train\": 32,\n",
    "        \"bs_val\": 32,\n",
    "        \"patience\": np.inf,\n",
    "        \"focal\": [1],\n",
    "        \"warm_up\": [],\n",
    "        \"scheduler\": lr_scheduler.StepLR,\n",
    "        \"scheduler_args\": {\"step_size\": 1, \"gamma\": 0.9},\n",
    "    },\n",
    "    \"transform_d\": {\n",
    "        (0,): transform_light,\n",
    "        (1,): transform_medium,\n",
    "        (): transform_dense,\n",
    "    },\n",
    "}\n",
    "\n",
    "model, predictions = train(**train_inp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vit_l](imgs/vit_l_16.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ViT-L_16 model is significantly slower than the ViT-B_16 model, this becomes obvious when looking at the amount of parameters that the two models have. In PyTorch, the ViT-B/16 model has 21M parameters and the ViT-L/16 model has 304M parameters, which is 14 times more than the ViT-B/16 model. \n",
    "\n",
    "Training for two epochs took around 20 minutes, and the prediction time was 11 minutes. But the result was an improvement over the ViT-B_16 model, with a score of 0.03063.\n",
    "\n",
    "I made a couple of training runs with different parameters, and the ones i used was the ones that gave the best results. So the two models dont use the exact same parameters, but instead the ones that gave the best results for each model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Main with CV***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my previous kaggle competition with tabular data, I used cross validation to get a better estimate of the models performance. I wanted to try the same thing with this competition, but I wasn't sure how to do it with images. I tried to implement it in a similar way as I did with tabular data, but with some slight modifications.\n",
    "\n",
    "I wanted to do a cross validation with 5 folds, and for each fold save the model state dict. Then after the cross validation is done, I will create an ensemble of the 5 models and use that for the final prediction. \n",
    "\n",
    "This would take a lot longer to preform, but I was curious to see if it would improve the results.\n",
    "\n",
    "\n",
    "As with the last training loop, this is not an optimal implementation and is done for experimental purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_train(\n",
    "    model_fn, data, epochs, deterministic, params, transform_d, n_splits=5\n",
    "):\n",
    "    start = time()\n",
    "    wd, lr, bs, bs_val, patience, focal, warm_up, *scheduler_init = params.values()\n",
    "\n",
    "    print(\n",
    "        f\"Parameters: {bl}batch_size={bs}, weight_decay={wd}\",\n",
    "        f\"learning_rate={lr}{rs}\\n\",\n",
    "    )\n",
    "\n",
    "    train_data, _, test_data = data\n",
    "    test_loader = get_loader(test_data, bs_val)\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "    models = []\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(train_data)):\n",
    "        print(f\"{gld}Fold {fold + 1}/{n_splits}{rs}\\n\")\n",
    "\n",
    "        set_seed() if deterministic else remove_seed()\n",
    "        train_data_fold = torch.utils.data.Subset(train_data, train_index)\n",
    "        val_data_fold = torch.utils.data.Subset(train_data, val_index)\n",
    "\n",
    "        train_loader = set_transform(train_data_fold, transform_d, fold, bs)\n",
    "        val_loader = get_loader(val_data_fold, bs_val)\n",
    "\n",
    "        model = model_fn()\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "        model, optimizer, scheduler = get_model(model, lr, wd, scheduler_init)\n",
    "\n",
    "        best_loss, best_state = np.inf, None\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"{wh}[Epoch {epoch+1}/{epochs}]{rs}\")\n",
    "\n",
    "            train_args = [model, criterion, optimizer, scaler]\n",
    "\n",
    "            one_epoch(train_loader, True, *train_args, epoch in focal)\n",
    "            val_loss = one_epoch(val_loader, False, *train_args)\n",
    "            empty_cache()\n",
    "\n",
    "            if scheduler is not None and epoch not in [warm_up, epochs - 1]:\n",
    "                scheduler.step()\n",
    "                print(f\"Updated lr -> {ma}{scheduler.get_last_lr()[0]:.2e}{rs}\\n\")\n",
    "\n",
    "            _, best_loss, best_state, early_stop = early_stopping(\n",
    "                0, best_loss, best_state, val_loss, model, patience, epoch\n",
    "            )\n",
    "\n",
    "            if early_stop and epoch != epochs - 1:\n",
    "                break\n",
    "\n",
    "        models.append(copy.deepcopy(model))\n",
    "\n",
    "        del model, optimizer, scaler, criterion, scheduler\n",
    "\n",
    "    ensemble = Ensemble(models)\n",
    "    predictions = make_predictions(ensemble, test_loader, bs_val)\n",
    "    submit_predictions(predictions)\n",
    "    empty_cache()\n",
    "\n",
    "    print(f\"\\n{bl}(ﾉ´ヮ`)ﾉ*  Finished in {(time() - start) / 60:.2f} minutes{rs}\")\n",
    "\n",
    "    return models, ensemble, predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Training with CV***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inp = {\n",
    "    \"train_amn\": 1,\n",
    "    \"val_amn\": 0,\n",
    "}\n",
    "\n",
    "data_vit_b_16_cv = get_data(**data_inp, model=\"vit_b_16\")\n",
    "data_vit_l_16_cv = get_data(**data_inp, model=\"vit_l_16\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***ViT-B_16***\n",
    "\n",
    "5 fold cross validation with 3 epochs for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: \u001b[34mbatch_size=32, weight_decay=5e-05 learning_rate=0.002\u001b[0m\n",
      "\n",
      "\u001b[33m\u001b[1mFold 1/5\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[1m[Epoch 1/3]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624/624 [00:46<00:00, 13.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624: [Train] Loss: \u001b[34m0.0116\u001b[0m, Accuracy: \u001b[34m0.9964\u001b[0m log_loss: \u001b[34m0.0152\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:40<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: \u001b[32m0.0083\u001b[0m, Accuracy: \u001b[32m0.9986\u001b[0m, Log Loss: \u001b[32m0.0109\u001b[0m\n",
      "\n",
      "Updated lr -> \u001b[35m1.40e-03\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[1m[Epoch 2/3]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624/624 [00:47<00:00, 13.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624: [Train] Loss: \u001b[34m0.0036\u001b[0m, Accuracy: \u001b[34m0.9990\u001b[0m log_loss: \u001b[34m0.0080\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:38<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: \u001b[32m0.0074\u001b[0m, Accuracy: \u001b[32m0.9984\u001b[0m, Log Loss: \u001b[32m0.0103\u001b[0m\n",
      "\n",
      "Updated lr -> \u001b[35m9.80e-04\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[1m[Epoch 3/3]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624/624 [00:47<00:00, 13.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624: [Train] Loss: \u001b[34m0.0007\u001b[0m, Accuracy: \u001b[34m0.9993\u001b[0m log_loss: \u001b[34m0.0073\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:38<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: \u001b[32m0.0076\u001b[0m, Accuracy: \u001b[32m0.9986\u001b[0m, Log Loss: \u001b[32m0.0105\u001b[0m\n",
      "\n",
      "\u001b[33m\u001b[1mFold 2/5\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[1m[Epoch 1/3]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624/624 [00:47<00:00, 13.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624: [Train] Loss: \u001b[34m0.0117\u001b[0m, Accuracy: \u001b[34m0.9968\u001b[0m log_loss: \u001b[34m0.0152\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:38<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: \u001b[32m0.0046\u001b[0m, Accuracy: \u001b[32m0.9986\u001b[0m, Log Loss: \u001b[32m0.0091\u001b[0m\n",
      "\n",
      "Updated lr -> \u001b[35m1.40e-03\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[1m[Epoch 2/3]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624/624 [00:47<00:00, 13.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624: [Train] Loss: \u001b[34m0.0043\u001b[0m, Accuracy: \u001b[34m0.9987\u001b[0m log_loss: \u001b[34m0.0085\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:38<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: \u001b[32m0.0043\u001b[0m, Accuracy: \u001b[32m0.9978\u001b[0m, Log Loss: \u001b[32m0.0089\u001b[0m\n",
      "\n",
      "Updated lr -> \u001b[35m9.80e-04\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[1m[Epoch 3/3]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624/624 [00:46<00:00, 13.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624: [Train] Loss: \u001b[34m0.0008\u001b[0m, Accuracy: \u001b[34m0.9993\u001b[0m log_loss: \u001b[34m0.0075\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:39<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: \u001b[32m0.0057\u001b[0m, Accuracy: \u001b[32m0.9982\u001b[0m, Log Loss: \u001b[32m0.0103\u001b[0m\n",
      "\n",
      "\u001b[33m\u001b[1mFold 3/5\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[1m[Epoch 1/3]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624/624 [00:47<00:00, 13.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624: [Train] Loss: \u001b[34m0.0110\u001b[0m, Accuracy: \u001b[34m0.9970\u001b[0m log_loss: \u001b[34m0.0146\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:39<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: \u001b[32m0.0099\u001b[0m, Accuracy: \u001b[32m0.9976\u001b[0m, Log Loss: \u001b[32m0.0134\u001b[0m\n",
      "\n",
      "Updated lr -> \u001b[35m1.40e-03\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[1m[Epoch 2/3]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624/624 [00:47<00:00, 13.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624: [Train] Loss: \u001b[34m0.0041\u001b[0m, Accuracy: \u001b[34m0.9988\u001b[0m log_loss: \u001b[34m0.0085\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:39<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: \u001b[32m0.0084\u001b[0m, Accuracy: \u001b[32m0.9984\u001b[0m, Log Loss: \u001b[32m0.0120\u001b[0m\n",
      "\n",
      "Updated lr -> \u001b[35m9.80e-04\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[1m[Epoch 3/3]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624/624 [00:47<00:00, 13.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624: [Train] Loss: \u001b[34m0.0006\u001b[0m, Accuracy: \u001b[34m0.9992\u001b[0m log_loss: \u001b[34m0.0067\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:39<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: \u001b[32m0.0090\u001b[0m, Accuracy: \u001b[32m0.9982\u001b[0m, Log Loss: \u001b[32m0.0126\u001b[0m\n",
      "\n",
      "\u001b[33m\u001b[1mFold 4/5\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[1m[Epoch 1/3]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624/624 [00:47<00:00, 13.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624: [Train] Loss: \u001b[34m0.0124\u001b[0m, Accuracy: \u001b[34m0.9966\u001b[0m log_loss: \u001b[34m0.0159\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:38<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: \u001b[32m0.0045\u001b[0m, Accuracy: \u001b[32m0.9986\u001b[0m, Log Loss: \u001b[32m0.0090\u001b[0m\n",
      "\n",
      "Updated lr -> \u001b[35m1.40e-03\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[1m[Epoch 2/3]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624/624 [00:46<00:00, 13.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624: [Train] Loss: \u001b[34m0.0040\u001b[0m, Accuracy: \u001b[34m0.9988\u001b[0m log_loss: \u001b[34m0.0083\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:38<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: \u001b[32m0.0040\u001b[0m, Accuracy: \u001b[32m0.9984\u001b[0m, Log Loss: \u001b[32m0.0086\u001b[0m\n",
      "\n",
      "Updated lr -> \u001b[35m9.80e-04\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[1m[Epoch 3/3]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624/624 [00:47<00:00, 13.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624: [Train] Loss: \u001b[34m0.0008\u001b[0m, Accuracy: \u001b[34m0.9994\u001b[0m log_loss: \u001b[34m0.0075\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:39<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: \u001b[32m0.0045\u001b[0m, Accuracy: \u001b[32m0.9982\u001b[0m, Log Loss: \u001b[32m0.0092\u001b[0m\n",
      "\n",
      "\u001b[33m\u001b[1mFold 5/5\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[1m[Epoch 1/3]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624/624 [00:47<00:00, 13.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624: [Train] Loss: \u001b[34m0.0135\u001b[0m, Accuracy: \u001b[34m0.9961\u001b[0m log_loss: \u001b[34m0.0168\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:39<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: \u001b[32m0.0040\u001b[0m, Accuracy: \u001b[32m0.9988\u001b[0m, Log Loss: \u001b[32m0.0085\u001b[0m\n",
      "\n",
      "Updated lr -> \u001b[35m1.40e-03\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[1m[Epoch 2/3]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624/624 [00:47<00:00, 13.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624: [Train] Loss: \u001b[34m0.0043\u001b[0m, Accuracy: \u001b[34m0.9987\u001b[0m log_loss: \u001b[34m0.0084\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:39<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: \u001b[32m0.0048\u001b[0m, Accuracy: \u001b[32m0.9982\u001b[0m, Log Loss: \u001b[32m0.0094\u001b[0m\n",
      "\n",
      "Updated lr -> \u001b[35m9.80e-04\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[1m[Epoch 3/3]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624/624 [00:47<00:00, 13.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624: [Train] Loss: \u001b[34m0.0008\u001b[0m, Accuracy: \u001b[34m0.9991\u001b[0m log_loss: \u001b[34m0.0075\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:39<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: \u001b[32m0.0040\u001b[0m, Accuracy: \u001b[32m0.9990\u001b[0m, Log Loss: \u001b[32m0.0086\u001b[0m\n",
      "\n",
      "Predicting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [07:24<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 137k/137k [00:00<00:00, 230kB/s]\n",
      "Successfully submitted to Dogs vs. Cats Redux: Kernels Edition\n",
      "\u001b[34m(ﾉ´ヮ`)ﾉ*  Finished in 29.13 minutes\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_inp = {\n",
    "    \"model_fn\": load_vit_b_16,\n",
    "    \"data\": data_vit_b_16_cv,\n",
    "    \"epochs\": 3,\n",
    "    \"deterministic\": False,\n",
    "    \"load_best\": (False, False),\n",
    "    \"params\": {\n",
    "        \"weight_decay\": 5e-5,\n",
    "        \"learning_rate\": 2e-3,\n",
    "        \"bs_train\": 32,\n",
    "        \"bs_val\": 64,\n",
    "        \"patience\": np.inf,\n",
    "        \"focal\": [2],\n",
    "        \"warm_up\": [0],\n",
    "        \"scheduler\": lr_scheduler.StepLR,\n",
    "        \"scheduler_args\": {\"step_size\": 1, \"gamma\": 0.7},\n",
    "    },\n",
    "    \"transform_d\": {\n",
    "        tuple(range(4, 4)): transform_light,\n",
    "        tuple(range(1, 2)): transform_medium,\n",
    "        tuple(range(16, 21)): transform_dense,\n",
    "    },\n",
    "}\n",
    "\n",
    "models, ensemble, predictions = cv_train(**train_inp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vit_b_cv](imgs/vit_b_16_cv_5fold.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total time for training and prediction was 29 minutes, and the score was 0.03162, which is an improvement over the previous ViT-B_16 prediction. Even though this score is a more robust score, that likley would be more accurate than the previous score, it still worth questioning if the extra time spent on training and prediction is worth it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***ViT-L_16***\n",
    "\n",
    "5 fold cross validation with 2 epochs for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: \u001b[34mbatch_size=32, weight_decay=0.001 learning_rate=0.002\u001b[0m\n",
      "\n",
      "\u001b[33m\u001b[1mFold 1/5\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[1m[Epoch 1/2]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624/624 [04:31<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624: [Train] Loss: \u001b[34m0.0082\u001b[0m, Accuracy: \u001b[34m0.9980\u001b[0m log_loss: \u001b[34m0.0121\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [03:59<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: \u001b[32m0.0039\u001b[0m, Accuracy: \u001b[32m0.9996\u001b[0m, Log Loss: \u001b[32m0.0073\u001b[0m\n",
      "\n",
      "Updated lr -> \u001b[35m1.80e-03\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[1m[Epoch 2/2]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624/624 [04:32<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624: [Train] Loss: \u001b[34m0.0005\u001b[0m, Accuracy: \u001b[34m0.9994\u001b[0m log_loss: \u001b[34m0.0067\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [03:58<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: \u001b[32m0.0035\u001b[0m, Accuracy: \u001b[32m0.9996\u001b[0m, Log Loss: \u001b[32m0.0073\u001b[0m\n",
      "\n",
      "\u001b[33m\u001b[1mFold 2/5\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[1m[Epoch 1/2]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624/624 [04:31<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624: [Train] Loss: \u001b[34m0.0125\u001b[0m, Accuracy: \u001b[34m0.9948\u001b[0m log_loss: \u001b[34m0.0161\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [03:59<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: \u001b[32m0.0013\u001b[0m, Accuracy: \u001b[32m0.9998\u001b[0m, Log Loss: \u001b[32m0.0058\u001b[0m\n",
      "\n",
      "Updated lr -> \u001b[35m1.80e-03\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[1m[Epoch 2/2]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624/624 [04:32<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624: [Train] Loss: \u001b[34m0.0006\u001b[0m, Accuracy: \u001b[34m0.9995\u001b[0m log_loss: \u001b[34m0.0067\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [03:58<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: \u001b[32m0.0012\u001b[0m, Accuracy: \u001b[32m0.9998\u001b[0m, Log Loss: \u001b[32m0.0057\u001b[0m\n",
      "\n",
      "\u001b[33m\u001b[1mFold 3/5\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[1m[Epoch 1/2]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624/624 [04:31<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624: [Train] Loss: \u001b[34m0.0089\u001b[0m, Accuracy: \u001b[34m0.9971\u001b[0m log_loss: \u001b[34m0.0125\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [03:58<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: \u001b[32m0.0059\u001b[0m, Accuracy: \u001b[32m0.9988\u001b[0m, Log Loss: \u001b[32m0.0101\u001b[0m\n",
      "\n",
      "Updated lr -> \u001b[35m1.80e-03\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[1m[Epoch 2/2]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624/624 [04:31<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624: [Train] Loss: \u001b[34m0.0007\u001b[0m, Accuracy: \u001b[34m0.9993\u001b[0m log_loss: \u001b[34m0.0068\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [03:58<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: \u001b[32m0.0051\u001b[0m, Accuracy: \u001b[32m0.9988\u001b[0m, Log Loss: \u001b[32m0.0094\u001b[0m\n",
      "\n",
      "\u001b[33m\u001b[1mFold 4/5\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[1m[Epoch 1/2]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624/624 [04:32<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624: [Train] Loss: \u001b[34m0.0099\u001b[0m, Accuracy: \u001b[34m0.9969\u001b[0m log_loss: \u001b[34m0.0136\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [03:59<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: \u001b[32m0.0018\u001b[0m, Accuracy: \u001b[32m0.9996\u001b[0m, Log Loss: \u001b[32m0.0063\u001b[0m\n",
      "\n",
      "Updated lr -> \u001b[35m1.80e-03\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[1m[Epoch 2/2]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624/624 [04:32<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624: [Train] Loss: \u001b[34m0.0005\u001b[0m, Accuracy: \u001b[34m0.9994\u001b[0m log_loss: \u001b[34m0.0066\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [03:59<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: \u001b[32m0.0013\u001b[0m, Accuracy: \u001b[32m0.9996\u001b[0m, Log Loss: \u001b[32m0.0059\u001b[0m\n",
      "\n",
      "\u001b[33m\u001b[1mFold 5/5\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[1m[Epoch 1/2]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624/624 [04:32<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624: [Train] Loss: \u001b[34m0.0074\u001b[0m, Accuracy: \u001b[34m0.9984\u001b[0m log_loss: \u001b[34m0.0111\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [03:59<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: \u001b[32m0.0054\u001b[0m, Accuracy: \u001b[32m0.9988\u001b[0m, Log Loss: \u001b[32m0.0091\u001b[0m\n",
      "\n",
      "Updated lr -> \u001b[35m1.80e-03\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[1m[Epoch 2/2]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624/624 [04:32<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624: [Train] Loss: \u001b[34m0.0006\u001b[0m, Accuracy: \u001b[34m0.9995\u001b[0m log_loss: \u001b[34m0.0066\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [03:59<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: \u001b[32m0.0056\u001b[0m, Accuracy: \u001b[32m0.9986\u001b[0m, Log Loss: \u001b[32m0.0096\u001b[0m\n",
      "\n",
      "Predicting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [49:10<00:00,  7.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 137k/137k [00:00<00:00, 233kB/s]\n",
      "Successfully submitted to Dogs vs. Cats Redux: Kernels Edition\n",
      "\u001b[34m(ﾉ´ヮ`)ﾉ*  Finished in 134.74 minutes\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_inp = {\n",
    "    \"model_fn\": load_vit_l_16,\n",
    "    \"data\": data_vit_l_16_cv,\n",
    "    \"epochs\": 2,\n",
    "    \"deterministic\": False,\n",
    "    \"load_best\": (False, False),\n",
    "    \"params\": {\n",
    "        \"weight_decay\": 1e-3,\n",
    "        \"learning_rate\": 2e-3,\n",
    "        \"bs_train\": 32,\n",
    "        \"bs_val\": 32,\n",
    "        \"patience\": np.inf,\n",
    "        \"focal\": [1],\n",
    "        \"warm_up\": [],\n",
    "        \"scheduler\": lr_scheduler.StepLR,\n",
    "        \"scheduler_args\": {\"step_size\": 1, \"gamma\": 0.9},\n",
    "    },\n",
    "    \"transform_d\": {\n",
    "        (0,): transform_light,\n",
    "        (1,): transform_medium,\n",
    "        (): transform_dense,\n",
    "    },\n",
    "}\n",
    "\n",
    "models, ensemble, predictions = cv_train(**train_inp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vit_l_cv](imgs/vit_l_16_cv_5fold.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total time was 2 hours and 15 minutes, and the score was 0.03056. This is the best score in this notebook but its only a 0.00010 improvement over the previous ViT-L_16 prediction. In this case, I don't think the extra time spent on training and prediction is worth it since the ViT-L_16 is already a quite robust model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for reading this notebook, I hope you found it interesting. If you have any feedback or suggestions, please let me by know!\n",
    "<center><img src=\"https://cdn.shopify.com/s/files/1/0100/8176/3385/products/59bb03a64ae09944b3f86bb9bcfdd8de_580x.jpg?v=1608234066\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
